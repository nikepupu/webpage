<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-VPZ41Q777R"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-VPZ41Q777R');
	</script>
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Xiaofeng&#39;s Blog">
    <meta name="author" content="Xiaofeng Gao">

    <title>Xiaofeng Gao</title>

    <!-- Bootstrap Core CSS -->
    <link href="./css/bootstrap.min.css" rel="stylesheet">

    <!-- Main CSS -->
    <link href="./css/main.css" rel="stylesheet">

    <!-- Font Awesome CSS -->
    <link rel="stylesheet" href="./css/font-awesome.min.css">

    <!-- icon -->
    <link rel="shortcut icon" href="imgs/xfgao.jpg" type="image/x-icon">
    <link rel="icon" href="imgs/xfgao.jpg" type="image/x-icon">

    <!-- Google Fonts -->
    <link rel="stylesheet" href="./css/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap scrollspy function -->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top navbar-inverse bg-inverse top-nav-collapse" role="navigation">
        <div class="container">
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="http://xfgao.github.io/#page-top">XIAOFENG GAO</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden active">
                        <a class="page-scroll" href="http://xfgao.github.io/#page-top"></a>
                    </li>
                    <li class="">
                        <a class="page-scroll" href="http://xfgao.github.io/#about">ABOUT</a>
                    </li>
<!--                     <li>
                        <a class="page-scroll" href="#projects">Projects</a>
                    </li> -->
                    <li class="">
                        <a class="page-scroll" href="http://xfgao.github.io/#publications">PUBLICATIONS</a>
                    </li>
                    <!-- <li>
                        <a class="page-scroll" href="#awards">Awards</a>
                    </li> -->
                    <li>
                        <a class="page-scroll" href="https://xfgao.github.io/CV_XiaofengGao_Jan2022.pdf">CV</a>
                    </li>
                </ul>
<!--                 <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="mailto:xfgao@ucla.edu" class="navbar-brand"><i class="fa fa-envelope"></i></a>
                    </li>
                    <li>
                        <a href="https://github.com/xfgao/" class="navbar-brand"><i class="fa fa-github"></i></a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=AjTfCjEAAAAJ&hl=en&authuser=1" class="navbar-brand"><i class="fa fa-graduation-cap"></i></a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/xiaofeng-gao-09b966115/" class="navbar-brand"><i class="fa fa-linkedin"></i></a>
                    </li>
                    <li>
                        <a href="https://www.facebook.com/profile.php?id=100010717656664" class="navbar-brand"><i class="fa fa-facebook"></i></a>
                    </li>

                </ul> -->

            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Section -->
    <section id="intro" class="intro-section">
        <div class="container">
            <div class="row">
                <div class="col-xs-12 col-sm-6 col-md-5 col-lg-4">
                        <img src="./imgs/xfgao.jpg" class="img-responsive img-circle img-fluid" width="250" align="right">
                </div>
                <div class="col-xs-12 col-md-6 col-md-7 col-lg-8 personal-intro">
                    <div class="col-xs-12 col-sm-12 col-md-12">
                        <h2 class="text-uppercase">Xiaofeng Gao</h2>
                        <h2><small>Ph.D. Candidate in Statistics at UCLA</small></h2>
                        <address>
                            Boelter Hall 9407<br>
                            580 Portola Plaza<br>
                            University of California, Los Angeles<br>
                            Los Angeles, CA, 90095
                        </address>
                        <span><strong>Email: </strong>xfgao at ucla dot edu</span>
                        <br>
                        <a href="https://scholar.google.com/citations?user=AjTfCjEAAAAJ&hl=en&authuser=1">[Google Scholar]</a>&nbsp;&nbsp;
						<a href="https://github.com/xfgao">[GitHub]</a>
                    </div>
					
                </div>
            </div>
        </div>
    </section>
    <hr>
    <!-- About Section -->
    <section id="about" class="about-section">
        <div class="container">
            <h1>About</h1>
            <div class="bio-info">
                <p>I am a fifth year Ph.D. candidate in the Department of Statistics, UCLA. </p>
                My research lies in the intersection of Robotics, Computer Vision, Machine Learning and Cognitive Science, with a focus on <strong>Human-Machine Interaction</strong> and <strong>Explainable AI</strong>. 

                <p> Currently I'm working in the <a href="http://vcla.stat.ucla.edu/">Center for Vision, Cognition, Learning, and Autonomy (VCLA)</a>, under the supervision of <a href="http://www.stat.ucla.edu/~sczhu/">Prof. Song-Chun Zhu</a>. I also work closely with <a href="https://www.psych.ucla.edu/faculty-page/hongjing/">Prof. Hongjing Lu</a> (UCLA), <a href="https://viterbi.usc.edu/directory/faculty/Sukhatme/Gaurav">Prof. Gaurav Sukhatme</a> (USC & Amazon Alexa AI) and <a href="http://tshu.io">Dr. Tianmin Shu</a> (MIT). Before that, I obtained a bachelor degree of Electronic Engineering at <a href="http://www.fudan.edu.cn/en/"> Fudan University.</a></p>
            </div>

        </div>
    </section>
    <hr>

    <!-- News Section -->
    <section id="news" class="news-section">
        <div class="container">
        	<h1>News</h1>
            <div class="news">
            	<p><strong>01/2022</strong>: Our paper "Show Me What You Can Do: Capability Calibration on Reachable Workspace for Human-Robot Collaboration" has been accepted by IEEE Robotics and Automation Letters. 
            	<p><strong>05/2021</strong>: Our paper "Show Me What You Can Do: Capability Calibration on Reachable Workspace for Human-Robot Collaboration" has been accepted for a <strong>spotlight</strong> presentation at the ICRA Workshop on Social Intelligence in Humans and Robots. <a href="https://social-intelligence-human-ai.github.io/">[Link]</a>
                <p><strong>01/2021</strong>: We presented our paper "Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks" at IJCAI 2020 Workshop on XAI. <a href="https://sites.google.com/view/xai2020/home">[Link]</a>
                <p><strong>01/2021</strong>: I started my internship at Honda Research Institute USA <a href="https://usa.honda-ri.com/">[Link]</a>. I would be working on projects related to Human-Machine Interaction. </p>
                <p><strong>07/2020</strong>: One paper got accepted by RO-MAN 2020. <a href="http://ro-man2020.unina.it/">[Link] </a></p>
            	<p><strong>05/2019</strong>: One paper got accepted by ICML workshop on Reinforcement Learning for Real Life. <a href="https://sites.google.com/view/RL4RealLife">[Link] </a></p>
            	<p><strong>03/2019</strong>: VRKitchen was covered by TechXplore. <a href="https://techxplore.com/news/2019-03-vrkitchen-interactive-virtual-environment-ai.html">[Link] </a> </p>
            	<p><strong>03/2019</strong>: I was invited as a reviewer for IROS 2019.</p>
            	<!-- <p><strong>03/2019</strong>: We submitted a paper to arXiv as a <a href="https://arxiv.org/pdf/1903.05757.pdf">technical report for VRKitchen</a>.</p> -->
            	<p><strong>03/2019</strong>: I passed the Oral Qualifying Exam and advanced to candidancy!</p>
            	<!-- <p><strong>02/2019</strong>: I gave a poster presentation at 
            		<a href="https://sites.google.com/go.spawar.navy.mil/naml/home"> the Third Annual Workshop on Naval Applications of Machine Learning </a>.</p>
            	<p><strong>09/2018</strong>: I was TAing for "<i>Stats 10: Introduction to Statistical Reasoning</i>", Fall 2018.</p>
                <p><strong>09/2017</strong>: I started my Ph.D. studies at UCLA.</p>
                <p><strong>03/2017</strong>: Our ICRA 2017 work was covered by New Scientist. <a href="https://www.newscientist.com/article/2129162-robots-taught-to-work-alongside-humans-by-giving-high-fives/">[Link] </a> </p> -->
            </div>
        </div>
    </section>
    <hr></hr>


    <section id="publications" class="publications-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1>Publications</h1>
                        <div class="contents">
                            <ul class="list-group">

                                <!-- Human-Robot Social Interaction -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/hri_intro.png" class="img-responsive img-fluid" width="150">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Learning Social Affordance Grammar from Videos: 
                                        	Transferring Human Interactions to Human-Robot Interactions 
                                        <span class="badge">
                                            <a class="conf" href="http://www.icra2017.org/">ICRA'17</a>
                                        </span></h4>
                                        <p class="detail">
                                            <a href="https://tshu.io">Tianmin Shu</a>,
                                            <strong>Xiaofeng Gao</strong>, 
                                            <a href="http://michaelryoo.com/">Michael S. Ryoo</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><br>
                                            <em>IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</em>, 2017
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#social_interaction-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://arxiv.org/pdf/1703.00503.pdf">PDF</a>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://www.tshu.io/SocialAffordanceGrammar/index.html">Website</a> 
                                        </div>
                                    </div>
                                    <div id="social_interaction-abs" class="collapse abstract">
                                        In this paper, we present a general framework for learning social affordance grammar as a spatiotemporal AND-OR graph (ST-AOG) from RGB-D videos of human interactions, and transfer the grammar to humanoids to enable a real-time motion inference for human-robot interaction (HRI). Based on Gibbs sampling, our weakly supervised grammar learning can automatically construct a hierarchical representation of an interaction with long-term joint sub-tasks of both agents and short term atomic actions of individual agents. Based on a new RGB-D video dataset with rich instances of human interactions, our experiments of Baxter simulation, human evaluation, and real Baxter test demonstrate that the model learned from limited training data successfully generates human-like behaviors in unseen scenarios and outperforms both baselines.
                                    </div>
                                </li>

                                <!-- VRKitchen -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/dish_fluent_small.gif" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">VRKitchen: an Interactive 3D Environment for Learning Real Life Cooking Tasks
                                        <span class="badge">
                                            <a class="conf" href="https://sites.google.com/view/RL4RealLife">RL4RealLife</a>
                                        </span></h4>
                                        <p class="detail">
                                            <strong>Xiaofeng Gao</strong>, 
                                            <a href="https://nikepupu.github.io/">Ran Gong</a>,
                                            <a href="https://tshu.io">Tianmin Shu</a>,
                                            <a href="https://xuxie1031.github.io/">Xu Xie</a>,
                                            <a href="https://shuwang0712.github.io/">Shu Wang</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><br>
                                            <em> ICML workshop on Reinforcement Learning for Real Life (<strong>RL4RealLife</strong>), 2019 </em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#kitchen-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://arxiv.org/pdf/1903.05757.pdf">PDF</a>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://sites.google.com/view/vr-kitchen/">Website</a> 
                                        </div>
                                    </div>
                                    <div id="kitchen-abs" class="collapse abstract">
                                        One of the main challenges of applying reinforcement learning to real world applications is the lack of realistic and standardized environments for training and testing AI agents. In this work, we design and implement a virtual reality (VR) system, VRKitchen, with integrated functions which i) enable embodied agents to perform real life cooking tasks involving a wide range of object manipulations and state changes, and ii) allow human teachers to provide demonstrations for training agents. We also provide standardized evaluation benchmarks and data collection tools to facilitate a broad use in research on learning real life tasks. Video demos, code, and data will be available on the project website: sites.google.com/view/vr-kitchen.
                                    </div>
                                </li>

                                <!-- XCooking -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/map_final.png" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks
                                        <span class="badge">
                                            <a class="conf" href="http://ro-man2020.unina.it/">RO-MAN'20</a>
                                        </span></h4>
                                        <p class="detail">
                                            <strong>Xiaofeng Gao</strong>, 
                                            <a href="https://nikepupu.github.io/">Ran Gong</a>,
                                            <a href="https://yizhouzhao.github.io/">Yizhou Zhao</a>,
                                            <a href="https://shuwang0712.github.io/">Shu Wang</a>,
                                            <a href="https://tshu.io">Tianmin Shu</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><br>
                                            <em> International Conference on Robot & Human Interactive Communication (<strong>RO-MAN</strong>), 2020 </em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#XCooking-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://arxiv.org/pdf/2007.12803.pdf">PDF</a>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://xfgao.github.io/xCookingWeb/">Website</a> 
                                        <a class="btn btn-warning btn-xs" role="button" href="https://youtu.be/Q8jmEQ6JqQ0">Talk</a>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://xfgao.github.io/slides/ROMAN20_collabCooking.pdf">Slides</a>
                                        </div>
                                    </div>
                                    <div id="XCooking-abs" class="collapse abstract">
                                        Human collaborators can effectively communicate with their partners to finish a common task by inferring each other's mental states (e.g., goals, beliefs, and desires). Such mind-aware communication minimizes the discrepancy among collaborators' mental states, and is crucial to the success in human ad-hoc teaming. We believe that robots collaborating with human users should demonstrate similar pedagogic behavior. Thus, in this paper, we propose a novel explainable AI (XAI) framework for achieving human-like communication in human-robot collaborations, where the robot builds a hierarchical mind model of the human user and generates explanations of its own mind as a form of communications based on its online Bayesian inference of the user's mental state. To evaluate our framework, we conduct a user study on a real-time human-robot cooking task. Experimental results show that the generated explanations of our approach significantly improves the collaboration performance and user perception of the robot.
                                    </div>
                                </li>

                                <!-- TIP attention -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/attention_intro.jpg" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Predicting Task-Driven Attention via Integrating Bottom-Up Stimulus and Top-Down Guidance
                                        <span class="badge">
                                            <a class="conf">TIP</a>
                                        </span></h4>
                                        <p class="detail">
                                            Zhixiong Nan,
                                            Jingjing Jiang,
                                            <strong>Xiaofeng Gao</strong>, 
                                            Sanping Zhou, 
                                            Weiliang Zuo,
                                            Ping Wei,
                                            Nanning Zheng <br>
                                            <em> IEEE Transactions on Image Processing (TIP), 2021</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#attention-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/Attention_TIP21.pdf">PDF</a>
                                        </div>
                                    </div>
                                    <div id="attention-abs" class="collapse abstract">
                                        Task-free attention has gained intensive interest in the computer vision community while relatively few works focus on task-driven attention (TDAttention). Thus this paper handles the problem of TDAttention prediction in daily scenarios where a human is doing a task. Motivated by the cognition mechanism that human attention allocation is jointly controlled by the top-down guidance and bottom-up stimulus, this paper proposes a cognitively-explanatory deep neural network model to predict TDAttention. Given an image sequence, bottom-up features, such as human pose and motion, are firstly extracted. At the same time, the coarse-grained task information and fine-grained task information are embedded as a top-down feature. The bottom-up features are then fused with the top-down feature to guide the model to predict TDAttention. Two public datasets are re-annotated to make them qualified for TDAttention prediction, and our model is widely compared with other models on the two datasets. In addition, some ablation studies are conducted to evaluate the individual modules in our model. Experiment results demonstrate the effectiveness of our model.
                                    </div>
                                </li>

                                <!-- Capability Calibration -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="./imgs/capcalib_intro.jpeg" class="img-responsive img-fluid">
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Show Me What You Can Do: Capability Calibration on Reachable Workspace for Human-Robot Collaboration
                                        <span class="badge">
                                            <a class="conf">RA-L</a>
                                        </span></h4>
                                        <p class="detail">
                                            <strong>Xiaofeng Gao</strong>, 
                                            <a href="https://yuanluya.github.io/">Luyao Yuan</a>,
                                            <a href="https://tshu.io">Tianmin Shu</a>,
                                            <a href="http://cvl.psych.ucla.edu/">Hongjing Lu</a>,
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><br>
                                            <em> IEEE Robotics and Automation Letters (RA-L), 2022</em>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target="#capcalib-abs">Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://xfgao.github.io/paper/calibration2022ral.pdf">PDF</a>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://xfgao.github.io/calibration2022ral">Website</a>
                                        <a class="btn btn-warning btn-xs" role="button" href="https://youtu.be/-dxtKtUt7Os">Talk</a>
                                        </div>
                                    </div>
                                    <div id="capcalib-abs" class="collapse abstract">
                                        Aligning humans' assessment of what a robot can do with its true capability is crucial for establishing a common ground between human and robot partners when they collaborate on a joint task. In this work, we propose an approach to calibrate humans' estimate of a robot's reachable workspace through a small number of demonstrations before collaboration. We develop a novel motion planning method, REMP (Reachability-Expressive Motion Planning), which jointly optimizes the physical cost and the expressiveness of robot motion to reveal the robot's motion capability to a human observer. Our experiments with human participants demonstrate that a short calibration using REMP can effectively bridge the gap between what a non-expert user thinks a robot can reach and the ground-truth. We show that this calibration procedure not only results in better user perception, but also promotes more efficient human-robot collaborations in a subsequent joint task.
                                    </div>
                                </li>
   
                            </ul>
                        </div>
                </div>
            </div>
        </div>
    </section>
    <hr>

    <footer class="footer">
        <div class="container">
            <p class="text-muted">© 2022 All rights reserved. Developed by Xiaofeng Gao.</p>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="./css/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="./css/bootstrap.min.js"></script>

    <!-- Scrolling Nav JavaScript -->
    <script src="./css/jquery.easing.min.js"></script>
    <script src="./css/scrolling-nav.js"></script>




</body></html>
